{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/un1tz3r0/finetunepixelartdiffusion/blob/main/Training_a_Fine_Tuned_Diffusion_Model_on_Isometric_Pixel_Art.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hBAjQO1kiEW"
      },
      "source": [
        "A simple colab to fine-tune openai diffusion models.\n",
        "\n",
        "With some modifications to use my (isometric pixelart dataset)[https://github.com/un1tz3r0/pixelscapes-dataset] and auto-resume from the most recent model snapshot found on google drive, or start with kaliyuga's soft pixel art diffusion model otherwise.\n",
        "\n",
        "Feel free to ask questions in this post's comments: https://www.patreon.com/posts/66246423 \n",
        "\n",
        "and questions specific to my pixelart fork here: https://github.com/un1tz3r0/finetunepixelartdiffusion \n",
        "\n",
        "by [Alex Spirin](https://twitter.com/devdef)\n",
        "\n",
        "iso pixelart fork by [Victor Condino](https://twitter.com/un1tz3r0)\n",
        "\n",
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=fine_tuning_openai_diffusion_model_on_iso_pixelart_ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ8BNzApp_Xk"
      },
      "source": [
        "#Train (tune) BEDROOM model :D\n",
        "Needs 16gb GPU RAM\n",
        "\n",
        "Works in colab pro and on kaggle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufaUo7olwoF0"
      },
      "source": [
        "## Setup (run once per session)\n",
        "\n",
        "This mounts your google drive for easier storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtMv2MEzSzjN",
        "outputId": "a9c95956-c05c-4a75-d9a9-62f567d87828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg3mlCMIe1B6"
      },
      "source": [
        "This downloads the training code and installs it, then downloads a pre-trained model that we will be tuning on our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-fL3fb8wpxZ",
        "outputId": "1ae8160f-e661-437d-87e4-7ef58c4f599b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'guided-diffusion-sxela'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 136 (delta 60), reused 104 (delta 51), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (136/136), 79.24 KiB | 7.20 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n",
            "/content/guided-diffusion-sxela\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/guided-diffusion-sxela\n",
            "Collecting blobfile>=1.0.5\n",
            "  Downloading blobfile-1.3.1-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from guided-diffusion==0.0.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from guided-diffusion==0.0.0) (4.64.0)\n",
            "Requirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.7/dist-packages (from blobfile>=1.0.5->guided-diffusion==0.0.0) (3.7.1)\n",
            "Collecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 31.5 MB/s \n",
            "\u001b[?25hCollecting urllib3~=1.25\n",
            "  Downloading urllib3-1.26.10-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 88.5 MB/s \n",
            "\u001b[?25hCollecting xmltodict~=0.12.0\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->guided-diffusion==0.0.0) (4.1.1)\n",
            "Installing collected packages: xmltodict, urllib3, pycryptodomex, blobfile, guided-diffusion\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Running setup.py develop for guided-diffusion\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.10 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed blobfile-1.3.1 guided-diffusion-0.0.0 pycryptodomex-3.15.0 urllib3-1.26.10 xmltodict-0.12.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Sxela/guided-diffusion-sxela\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd7VbOVO3626"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bECnYt6KXn8y",
        "outputId": "20b693e7-2077-4056-9165-bda78eabf493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating missing dataset from github repo now!\n",
            "/content\n",
            "Cloning into 'pixelscapes-dataset'...\n",
            "remote: Enumerating objects: 193, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (181/181), done.\u001b[K\n",
            "remote: Total 193 (delta 18), reused 181 (delta 10), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (193/193), 23.89 MiB | 22.90 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "/content\n",
            ">>> Installing upscaler network to zoom 2x source images\n",
            "/content\n",
            "Cloning into 'Real-ESRGAN'...\n",
            "remote: Enumerating objects: 682, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 682 (delta 4), reused 11 (delta 1), pack-reused 665\u001b[K\n",
            "Receiving objects: 100% (682/682), 5.03 MiB | 11.79 MiB/s, done.\n",
            "Resolving deltas: 100% (359/359), done.\n",
            "/content/Real-ESRGAN\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting basicsr>=1.3.3.11\n",
            "  Downloading basicsr-1.3.5.tar.gz (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 14.7 MB/s \n",
            "\u001b[?25hCollecting facexlib>=0.2.0.3\n",
            "  Downloading facexlib-0.2.4-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting gfpgan>=0.2.1\n",
            "  Downloading gfpgan-1.3.2-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.1.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.12.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (4.64.0)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.3.3.11->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.3.3.11->-r requirements.txt (line 1)) (0.99)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.3.3.11->-r requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.3.3.11->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.3.3.11->-r requirements.txt (line 1)) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from basicsr>=1.3.3.11->-r requirements.txt (line 1)) (1.4.1)\n",
            "Collecting tb-nightly\n",
            "  Downloading tb_nightly-2.10.0a20220711-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 84.4 MB/s \n",
            "\u001b[?25hCollecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 100.2 MB/s \n",
            "\u001b[?25hCollecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 94.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from facexlib>=0.2.0.3->-r requirements.txt (line 2)) (0.51.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 63.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->-r requirements.txt (line 7)) (4.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from filterpy->facexlib>=0.2.0.3->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.0.3->-r requirements.txt (line 2)) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.0.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.0.3->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->filterpy->facexlib>=0.2.0.3->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy->facexlib>=0.2.0.3->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.0.3->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->facexlib>=0.2.0.3->-r requirements.txt (line 2)) (0.34.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (2022.6.15)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 103.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (1.46.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (3.3.7)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly->basicsr>=1.3.3.11->-r requirements.txt (line 1)) (3.2.0)\n",
            "Building wheels for collected packages: basicsr, filterpy\n",
            "  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for basicsr: filename=basicsr-1.3.5-py3-none-any.whl size=194483 sha256=b1b5326f90d5e17d3d5d4f8ee786d407e82480df8db47854204d68d51620a671\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/1b/d0/8659cf028233dd1e3bf282271009fbf037dfc4ab761f32a032\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=08df007739e44f73fc3539334cdd07902e181fe55e27462ebfeda1e13d5700f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n",
            "Successfully built basicsr filterpy\n",
            "Installing collected packages: urllib3, numpy, yapf, tb-nightly, filterpy, addict, facexlib, basicsr, gfpgan\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.10\n",
            "    Uninstalling urllib3-1.26.10:\n",
            "      Successfully uninstalled urllib3-1.26.10\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.20.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed addict-2.4.0 basicsr-1.3.5 facexlib-0.2.4 filterpy-1.4.5 gfpgan-1.3.2 numpy-1.20.3 tb-nightly-2.10.0a20220711 urllib3-1.25.11 yapf-0.32.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running develop\n",
            "running egg_info\n",
            "creating realesrgan.egg-info\n",
            "writing realesrgan.egg-info/PKG-INFO\n",
            "writing dependency_links to realesrgan.egg-info/dependency_links.txt\n",
            "writing requirements to realesrgan.egg-info/requires.txt\n",
            "writing top-level names to realesrgan.egg-info/top_level.txt\n",
            "writing manifest file 'realesrgan.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'realesrgan.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /root/.local/lib/python3.7/site-packages/realesrgan.egg-link (link to .)\n",
            "Adding realesrgan 0.2.5.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/Real-ESRGAN\n",
            "Processing dependencies for realesrgan==0.2.5.0\n",
            "Searching for tqdm==4.64.0\n",
            "Best match: tqdm 4.64.0\n",
            "Adding tqdm 4.64.0 to easy-install.pth file\n",
            "Installing tqdm script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torchvision==0.12.0+cu113\n",
            "Best match: torchvision 0.12.0+cu113\n",
            "Adding torchvision 0.12.0+cu113 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for torch==1.11.0+cu113\n",
            "Best match: torch 1.11.0+cu113\n",
            "Adding torch 1.11.0+cu113 to easy-install.pth file\n",
            "Installing convert-caffe2-to-onnx script to /root/.local/bin\n",
            "Installing convert-onnx-to-caffe2 script to /root/.local/bin\n",
            "Installing torchrun script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Pillow==7.1.2\n",
            "Best match: Pillow 7.1.2\n",
            "Adding Pillow 7.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for opencv-python==4.1.2.30\n",
            "Best match: opencv-python 4.1.2.30\n",
            "Adding opencv-python 4.1.2.30 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.20.3\n",
            "Best match: numpy 1.20.3\n",
            "Adding numpy 1.20.3 to easy-install.pth file\n",
            "Installing f2py script to /root/.local/bin\n",
            "Installing f2py3 script to /root/.local/bin\n",
            "Installing f2py3.7 script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for gfpgan==1.3.2\n",
            "Best match: gfpgan 1.3.2\n",
            "Adding gfpgan 1.3.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for facexlib==0.2.4\n",
            "Best match: facexlib 0.2.4\n",
            "Adding facexlib 0.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for basicsr==1.3.5\n",
            "Best match: basicsr 1.3.5\n",
            "Adding basicsr 1.3.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for typing-extensions==4.1.1\n",
            "Best match: typing-extensions 4.1.1\n",
            "Adding typing-extensions 4.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for PyYAML==3.13\n",
            "Best match: PyYAML 3.13\n",
            "Adding PyYAML 3.13 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tb-nightly==2.10.0a20220711\n",
            "Best match: tb-nightly 2.10.0a20220711\n",
            "Adding tb-nightly 2.10.0a20220711 to easy-install.pth file\n",
            "Installing tensorboard script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for lmdb==0.99\n",
            "Best match: lmdb 0.99\n",
            "Adding lmdb 0.99 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for yapf==0.32.0\n",
            "Best match: yapf 0.32.0\n",
            "Adding yapf 0.32.0 to easy-install.pth file\n",
            "Installing yapf script to /root/.local/bin\n",
            "Installing yapf-diff script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for filterpy==1.4.5\n",
            "Best match: filterpy 1.4.5\n",
            "Adding filterpy 1.4.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numba==0.51.2\n",
            "Best match: numba 0.51.2\n",
            "Adding numba 0.51.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for addict==2.4.0\n",
            "Best match: addict 2.4.0\n",
            "Adding addict 2.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for scikit-image==0.18.3\n",
            "Best match: scikit-image 0.18.3\n",
            "Adding scikit-image 0.18.3 to easy-install.pth file\n",
            "Installing skivi script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for future==0.16.0\n",
            "Best match: future 0.16.0\n",
            "Adding future 0.16.0 to easy-install.pth file\n",
            "Installing futurize script to /root/.local/bin\n",
            "Installing pasteurize script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for urllib3==1.25.11\n",
            "Best match: urllib3 1.25.11\n",
            "Adding urllib3 1.25.11 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for certifi==2022.6.15\n",
            "Best match: certifi 2022.6.15\n",
            "Adding certifi 2022.6.15 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for google-auth==1.35.0\n",
            "Best match: google-auth 1.35.0\n",
            "Adding google-auth 1.35.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tensorboard-data-server==0.6.1\n",
            "Best match: tensorboard-data-server 0.6.1\n",
            "Adding tensorboard-data-server 0.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for wheel==0.37.1\n",
            "Best match: wheel 0.37.1\n",
            "Adding wheel 0.37.1 to easy-install.pth file\n",
            "Installing wheel script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for grpcio==1.46.3\n",
            "Best match: grpcio 1.46.3\n",
            "Adding grpcio 1.46.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tensorboard-plugin-wit==1.8.1\n",
            "Best match: tensorboard-plugin-wit 1.8.1\n",
            "Adding tensorboard-plugin-wit 1.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for absl-py==1.1.0\n",
            "Best match: absl-py 1.1.0\n",
            "Adding absl-py 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for google-auth-oauthlib==0.4.6\n",
            "Best match: google-auth-oauthlib 0.4.6\n",
            "Adding google-auth-oauthlib 0.4.6 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for setuptools==57.4.0\n",
            "Best match: setuptools 57.4.0\n",
            "Adding setuptools 57.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for protobuf==3.17.3\n",
            "Best match: protobuf 3.17.3\n",
            "Adding protobuf 3.17.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for Markdown==3.3.7\n",
            "Best match: Markdown 3.3.7\n",
            "Adding Markdown 3.3.7 to easy-install.pth file\n",
            "Installing markdown_py script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for matplotlib==3.2.2\n",
            "Best match: matplotlib 3.2.2\n",
            "Adding matplotlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for llvmlite==0.34.0\n",
            "Best match: llvmlite 0.34.0\n",
            "Adding llvmlite 0.34.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for imageio==2.4.1\n",
            "Best match: imageio 2.4.1\n",
            "Adding imageio 2.4.1 to easy-install.pth file\n",
            "Installing imageio_download_bin script to /root/.local/bin\n",
            "Installing imageio_remove_bin script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for PyWavelets==1.3.0\n",
            "Best match: PyWavelets 1.3.0\n",
            "Adding PyWavelets 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for tifffile==2021.11.2\n",
            "Best match: tifffile 2021.11.2\n",
            "Adding tifffile 2021.11.2 to easy-install.pth file\n",
            "Installing lsm2bin script to /root/.local/bin\n",
            "Installing tiff2fsspec script to /root/.local/bin\n",
            "Installing tiffcomment script to /root/.local/bin\n",
            "Installing tifffile script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for networkx==2.6.3\n",
            "Best match: networkx 2.6.3\n",
            "Adding networkx 2.6.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for rsa==4.8\n",
            "Best match: rsa 4.8\n",
            "Adding rsa 4.8 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /root/.local/bin\n",
            "Installing pyrsa-encrypt script to /root/.local/bin\n",
            "Installing pyrsa-keygen script to /root/.local/bin\n",
            "Installing pyrsa-priv2pub script to /root/.local/bin\n",
            "Installing pyrsa-sign script to /root/.local/bin\n",
            "Installing pyrsa-verify script to /root/.local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cachetools==4.2.4\n",
            "Best match: cachetools 4.2.4\n",
            "Adding cachetools 4.2.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for requests-oauthlib==1.3.1\n",
            "Best match: requests-oauthlib 1.3.1\n",
            "Adding requests-oauthlib 1.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for importlib-metadata==4.11.4\n",
            "Best match: importlib-metadata 4.11.4\n",
            "Adding importlib-metadata 4.11.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for kiwisolver==1.4.3\n",
            "Best match: kiwisolver 1.4.3\n",
            "Adding kiwisolver 1.4.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for cycler==0.11.0\n",
            "Best match: cycler 0.11.0\n",
            "Adding cycler 0.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyparsing==3.0.9\n",
            "Best match: pyparsing 3.0.9\n",
            "Adding pyparsing 3.0.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for oauthlib==3.2.0\n",
            "Best match: oauthlib 3.2.0\n",
            "Adding oauthlib 3.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for zipp==3.8.0\n",
            "Best match: zipp 3.8.0\n",
            "Adding zipp 3.8.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for realesrgan==0.2.5.0\n",
            "--2022-07-11 15:35:07--  https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/387326890/08f0e941-ebb7-48f0-9d6a-73e87b710e7e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220711%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220711T153507Z&X-Amz-Expires=300&X-Amz-Signature=107c8aab30871840b585babc59e0a947bdfe38235bbe956b3d2af5aa6e64d825&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=387326890&response-content-disposition=attachment%3B%20filename%3DRealESRGAN_x4plus.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-07-11 15:35:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/387326890/08f0e941-ebb7-48f0-9d6a-73e87b710e7e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220711%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220711T153507Z&X-Amz-Expires=300&X-Amz-Signature=107c8aab30871840b585babc59e0a947bdfe38235bbe956b3d2af5aa6e64d825&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=387326890&response-content-disposition=attachment%3B%20filename%3DRealESRGAN_x4plus.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67040989 (64M) [application/octet-stream]\n",
            "Saving to: ‘experiments/pretrained_models/RealESRGAN_x4plus.pth’\n",
            "\n",
            "RealESRGAN_x4plus.p 100%[===================>]  63.93M  29.3MB/s    in 2.2s    \n",
            "\n",
            "2022-07-11 15:35:10 (29.3 MB/s) - ‘experiments/pretrained_models/RealESRGAN_x4plus.pth’ saved [67040989/67040989]\n",
            "\n",
            ">>> Upscaling raw dataset images...\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/phunkUT-k24.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/phunkUT-k24.png' -> '/content/pixelscapes-dataset/pixelscapes/phunkUT-k24.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/SHP-village-reconstructed-05k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/SHP-village-reconstructed-05k.png' -> '/content/pixelscapes-dataset/pixelscapes/SHP-village-reconstructed-05k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/STO-PS3-Superhero-Background-23k-RGB.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/STO-PS3-Superhero-Background-23k-RGB.png' -> '/content/pixelscapes-dataset/pixelscapes/STO-PS3-Superhero-Background-23k-RGB.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/FAC-miami-entw-k15.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/FAC-miami-entw-k15.png' -> '/content/pixelscapes-dataset/pixelscapes/FAC-miami-entw-k15.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Cologne-Poster-V2-32t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Cologne-Poster-V2-32t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Cologne-Poster-V2-32t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Tokyo-V4-69t-randlos.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Tokyo-V4-69t-randlos.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Tokyo-V4-69t-randlos.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/SDZ-BER-Flughafen-22t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/SDZ-BER-Flughafen-22t.png' -> '/content/pixelscapes-dataset/pixelscapes/SDZ-BER-Flughafen-22t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-MarseilleAccents-05t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-MarseilleAccents-05t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-MarseilleAccents-05t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/KHT-ArlaSnow-11t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/KHT-ArlaSnow-11t.png' -> '/content/pixelscapes-dataset/pixelscapes/KHT-ArlaSnow-11t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/FRD-Bonn-20s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/FRD-Bonn-20s.png' -> '/content/pixelscapes-dataset/pixelscapes/FRD-Bonn-20s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/PTN-presidentiallibrary-14k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/PTN-presidentiallibrary-14k.png' -> '/content/pixelscapes-dataset/pixelscapes/PTN-presidentiallibrary-14k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/LAT-Oscars-11t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/LAT-Oscars-11t.png' -> '/content/pixelscapes-dataset/pixelscapes/LAT-Oscars-11t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Berlin-V4-81t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Berlin-V4-81t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Berlin-V4-81t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/NIC-Parallax-NICK-logo-devel-57k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/NIC-Parallax-NICK-logo-devel-57k.png' -> '/content/pixelscapes-dataset/pixelscapes/NIC-Parallax-NICK-logo-devel-57k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/MCS-singapore-19t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/MCS-singapore-19t.png' -> '/content/pixelscapes-dataset/pixelscapes/MCS-singapore-19t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-MakePoster-22t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-MakePoster-22t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-MakePoster-22t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/N73-GA-albumposter-09k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/N73-GA-albumposter-09k.png' -> '/content/pixelscapes-dataset/pixelscapes/N73-GA-albumposter-09k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/ICA-Osaka-88k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/ICA-Osaka-88k.png' -> '/content/pixelscapes-dataset/pixelscapes/ICA-Osaka-88k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-BaltimorePoster-66k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-BaltimorePoster-66k.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-BaltimorePoster-66k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/SWK-Städtische-Werke-Kassel-13t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/SWK-Städtische-Werke-Kassel-13t.png' -> '/content/pixelscapes-dataset/pixelscapes/SWK-Städtische-Werke-Kassel-13t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/KAI-Mixc-World-Shenzhen-64s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/KAI-Mixc-World-Shenzhen-64s.png' -> '/content/pixelscapes-dataset/pixelscapes/KAI-Mixc-World-Shenzhen-64s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/ECB-London-V5-40t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/ECB-London-V5-40t.png' -> '/content/pixelscapes-dataset/pixelscapes/ECB-London-V5-40t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/CAM-sugo-replay-k10.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/CAM-sugo-replay-k10.png' -> '/content/pixelscapes-dataset/pixelscapes/CAM-sugo-replay-k10.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/ECB-Superbroncobattle-14t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/ECB-Superbroncobattle-14t.png' -> '/content/pixelscapes-dataset/pixelscapes/ECB-Superbroncobattle-14t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-amnesty-30t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-amnesty-30t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-amnesty-30t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Wallpaper-Atlantis-18t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Wallpaper-Atlantis-18t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Wallpaper-Atlantis-18t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-MarseillePoster-30k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-MarseillePoster-30k.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-MarseillePoster-30k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/CPR-Campra-Porsche-28t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/CPR-Campra-Porsche-28t.png' -> '/content/pixelscapes-dataset/pixelscapes/CPR-Campra-Porsche-28t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/GNT-Superheroe-Poster-16t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/GNT-Superheroe-Poster-16t.png' -> '/content/pixelscapes-dataset/pixelscapes/GNT-Superheroe-Poster-16t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/HTM-Hornbach-13s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/HTM-Hornbach-13s.png' -> '/content/pixelscapes-dataset/pixelscapes/HTM-Hornbach-13s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/ORM-MakePoster2008-14t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/ORM-MakePoster2008-14t.png' -> '/content/pixelscapes-dataset/pixelscapes/ORM-MakePoster2008-14t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/ECB-Assembler-18k-FLAT-RGB.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/ECB-Assembler-18k-FLAT-RGB.png' -> '/content/pixelscapes-dataset/pixelscapes/ECB-Assembler-18k-FLAT-RGB.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/MAG-Venedigposter-58k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/MAG-Venedigposter-58k.png' -> '/content/pixelscapes-dataset/pixelscapes/MAG-Venedigposter-58k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Marseille2013-40s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Marseille2013-40s.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Marseille2013-40s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/KHT-Arla-06s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/KHT-Arla-06s.png' -> '/content/pixelscapes-dataset/pixelscapes/KHT-Arla-06s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/FTN-glbl500cover-11t-2.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/FTN-glbl500cover-11t-2.png' -> '/content/pixelscapes-dataset/pixelscapes/FTN-glbl500cover-11t-2.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EWY-redcarpet-15t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EWY-redcarpet-15t.png' -> '/content/pixelscapes-dataset/pixelscapes/EWY-redcarpet-15t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/KBM-oslocentral-30s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/KBM-oslocentral-30s.png' -> '/content/pixelscapes-dataset/pixelscapes/KBM-oslocentral-30s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/JMC-CokeEcoAuctionRGB-17t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/JMC-CokeEcoAuctionRGB-17t.png' -> '/content/pixelscapes-dataset/pixelscapes/JMC-CokeEcoAuctionRGB-17t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/LEE-skybox-12k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/LEE-skybox-12k.png' -> '/content/pixelscapes-dataset/pixelscapes/LEE-skybox-12k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/MRS-MIA-Miami-Airport-96k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/MRS-MIA-Miami-Airport-96k.png' -> '/content/pixelscapes-dataset/pixelscapes/MRS-MIA-Miami-Airport-96k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/BFG-N-ergie-Firmenzentrale-05s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/BFG-N-ergie-Firmenzentrale-05s.png' -> '/content/pixelscapes-dataset/pixelscapes/BFG-N-ergie-Firmenzentrale-05s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/MCN-CokeCork-24s-CROP01.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/MCN-CokeCork-24s-CROP01.png' -> '/content/pixelscapes-dataset/pixelscapes/MCN-CokeCork-24s-CROP01.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/ATK-centralfestivaleast-mall-31t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/ATK-centralfestivaleast-mall-31t.png' -> '/content/pixelscapes-dataset/pixelscapes/ATK-centralfestivaleast-mall-31t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/CCN-Trafigura-white-papers-cover-14t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/CCN-Trafigura-white-papers-cover-14t.png' -> '/content/pixelscapes-dataset/pixelscapes/CCN-Trafigura-white-papers-cover-14t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/BNM-bostonmag-22s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/BNM-bostonmag-22s.png' -> '/content/pixelscapes-dataset/pixelscapes/BNM-bostonmag-22s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/CP9-ShopVille-53s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/CP9-ShopVille-53s.png' -> '/content/pixelscapes-dataset/pixelscapes/CP9-ShopVille-53s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/RBM-Diggin-Season2-25k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/RBM-Diggin-Season2-25k.png' -> '/content/pixelscapes-dataset/pixelscapes/RBM-Diggin-Season2-25k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/MCN-CokeDublin-43s-CROP01.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/MCN-CokeDublin-43s-CROP01.png' -> '/content/pixelscapes-dataset/pixelscapes/MCN-CokeDublin-43s-CROP01.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/JWT-station-14k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/JWT-station-14k.png' -> '/content/pixelscapes-dataset/pixelscapes/JWT-station-14k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/STR-CreationLuanda-2k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/STR-CreationLuanda-2k.png' -> '/content/pixelscapes-dataset/pixelscapes/STR-CreationLuanda-2k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/TM3-Home-04s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/TM3-Home-04s.png' -> '/content/pixelscapes-dataset/pixelscapes/TM3-Home-04s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/BFG-N-ergie-Heizkraftwerk-10s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/BFG-N-ergie-Heizkraftwerk-10s.png' -> '/content/pixelscapes-dataset/pixelscapes/BFG-N-ergie-Heizkraftwerk-10s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/MRS-Miami-Pixorama-79t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/MRS-Miami-Pixorama-79t.png' -> '/content/pixelscapes-dataset/pixelscapes/MRS-Miami-Pixorama-79t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/FTN-FutureNow-2017-cover-18k-half.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/FTN-FutureNow-2017-cover-18k-half.png' -> '/content/pixelscapes-dataset/pixelscapes/FTN-FutureNow-2017-cover-18k-half.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/PT-SueddeutscheZ-FacebookCampus-08t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/PT-SueddeutscheZ-FacebookCampus-08t.png' -> '/content/pixelscapes-dataset/pixelscapes/PT-SueddeutscheZ-FacebookCampus-08t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/LVT-Panoramic-Meiji-Shrine-14s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/LVT-Panoramic-Meiji-Shrine-14s.png' -> '/content/pixelscapes-dataset/pixelscapes/LVT-Panoramic-Meiji-Shrine-14s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/KHT-ArlaFarm-Winter-05t-io.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/KHT-ArlaFarm-Winter-05t-io.png' -> '/content/pixelscapes-dataset/pixelscapes/KHT-ArlaFarm-Winter-05t-io.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/WUN-xmas-14k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/WUN-xmas-14k.png' -> '/content/pixelscapes-dataset/pixelscapes/WUN-xmas-14k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-SanFrancisco-190k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-SanFrancisco-190k.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-SanFrancisco-190k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-ParisPoster-64t-1x.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-ParisPoster-64t-1x.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-ParisPoster-64t-1x.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/OGY-Yahoo-Poster-38t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/OGY-Yahoo-Poster-38t.png' -> '/content/pixelscapes-dataset/pixelscapes/OGY-Yahoo-Poster-38t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/MCN-CokeGalway-39s-CROP01.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/MCN-CokeGalway-39s-CROP01.png' -> '/content/pixelscapes-dataset/pixelscapes/MCN-CokeGalway-39s-CROP01.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/MCN-CokeBelfast-33s-CROP01.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/MCN-CokeBelfast-33s-CROP01.png' -> '/content/pixelscapes-dataset/pixelscapes/MCN-CokeBelfast-33s-CROP01.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Moscow-227k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Moscow-227k.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Moscow-227k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/FTU-Yuzutown-68k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/FTU-Yuzutown-68k.png' -> '/content/pixelscapes-dataset/pixelscapes/FTU-Yuzutown-68k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Wallpaper-Pixorama-41t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Wallpaper-Pixorama-41t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Wallpaper-Pixorama-41t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/KBM-oslocentralXmas-34t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/KBM-oslocentralXmas-34t.png' -> '/content/pixelscapes-dataset/pixelscapes/KBM-oslocentralXmas-34t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/TNY-holidaytrainshow-22k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/TNY-holidaytrainshow-22k.png' -> '/content/pixelscapes-dataset/pixelscapes/TNY-holidaytrainshow-22k.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-FooBar-35t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-FooBar-35t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-FooBar-35t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/LVT-Panoramic-TiliX-Omotesando-48t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/LVT-Panoramic-TiliX-Omotesando-48t.png' -> '/content/pixelscapes-dataset/pixelscapes/LVT-Panoramic-TiliX-Omotesando-48t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Wallpaper-Cars-13t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Wallpaper-Cars-13t.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Wallpaper-Cars-13t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/ECB-NY-V5-03t.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/ECB-NY-V5-03t.png' -> '/content/pixelscapes-dataset/pixelscapes/ECB-NY-V5-03t.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/TM3-Battlefield-12s.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/TM3-Battlefield-12s.png' -> '/content/pixelscapes-dataset/pixelscapes/TM3-Battlefield-12s.png'\n",
            "Upscaling x1.0: /content/pixelscapes-dataset/pixelscapes/EBY-Rio-Poster-34k.png -> /content/pixelscapes-dataset/scaled\n",
            "/content/Real-ESRGAN\n",
            "'/content/pixelscapes-dataset/scaled/EBY-Rio-Poster-34k.png' -> '/content/pixelscapes-dataset/pixelscapes/EBY-Rio-Poster-34k.png'\n",
            ">>> Done, now cropping from upscaled images...\n",
            "/content\n",
            "\n",
            "Skipped 0 of 75 images... weights for images based on area:\n",
            "   1.2 pixelscapes-dataset/scaled/phunkUT-k24.png\n",
            "   1.1 pixelscapes-dataset/scaled/SHP-village-reconstructed-05k.png\n",
            "   1.2 pixelscapes-dataset/scaled/STO-PS3-Superhero-Background-23k-RGB.png\n",
            "   1.3 pixelscapes-dataset/scaled/FAC-miami-entw-k15.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-Cologne-Poster-V2-32t.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-Tokyo-V4-69t-randlos.png\n",
            "   1.2 pixelscapes-dataset/scaled/SDZ-BER-Flughafen-22t.png\n",
            "   1.3 pixelscapes-dataset/scaled/EBY-MarseilleAccents-05t.png\n",
            "   1.1 pixelscapes-dataset/scaled/KHT-ArlaSnow-11t.png\n",
            "   1.4 pixelscapes-dataset/scaled/FRD-Bonn-20s.png\n",
            "   1.2 pixelscapes-dataset/scaled/PTN-presidentiallibrary-14k.png\n",
            "   1.2 pixelscapes-dataset/scaled/LAT-Oscars-11t.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-Berlin-V4-81t.png\n",
            "   1.2 pixelscapes-dataset/scaled/NIC-Parallax-NICK-logo-devel-57k.png\n",
            "   1.4 pixelscapes-dataset/scaled/MCS-singapore-19t.png\n",
            "   1.2 pixelscapes-dataset/scaled/EBY-MakePoster-22t.png\n",
            "   1.5 pixelscapes-dataset/scaled/N73-GA-albumposter-09k.png\n",
            "   1.4 pixelscapes-dataset/scaled/ICA-Osaka-88k.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-BaltimorePoster-66k.png\n",
            "   1.1 pixelscapes-dataset/scaled/SWK-Städtische-Werke-Kassel-13t.png\n",
            "   1.4 pixelscapes-dataset/scaled/KAI-Mixc-World-Shenzhen-64s.png\n",
            "   1.4 pixelscapes-dataset/scaled/ECB-London-V5-40t.png\n",
            "   1.2 pixelscapes-dataset/scaled/CAM-sugo-replay-k10.png\n",
            "   1.4 pixelscapes-dataset/scaled/ECB-Superbroncobattle-14t.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-amnesty-30t.png\n",
            "   1.2 pixelscapes-dataset/scaled/EBY-Wallpaper-Atlantis-18t.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-MarseillePoster-30k.png\n",
            "   1.2 pixelscapes-dataset/scaled/CPR-Campra-Porsche-28t.png\n",
            "   1.2 pixelscapes-dataset/scaled/GNT-Superheroe-Poster-16t.png\n",
            "   1.4 pixelscapes-dataset/scaled/HTM-Hornbach-13s.png\n",
            "   1.2 pixelscapes-dataset/scaled/ORM-MakePoster2008-14t.png\n",
            "   1.4 pixelscapes-dataset/scaled/ECB-Assembler-18k-FLAT-RGB.png\n",
            "   1.4 pixelscapes-dataset/scaled/MAG-Venedigposter-58k.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-Marseille2013-40s.png\n",
            "   1.1 pixelscapes-dataset/scaled/KHT-Arla-06s.png\n",
            "   1.2 pixelscapes-dataset/scaled/FTN-glbl500cover-11t-2.png\n",
            "   1.2 pixelscapes-dataset/scaled/EWY-redcarpet-15t.png\n",
            "   1.4 pixelscapes-dataset/scaled/KBM-oslocentral-30s.png\n",
            "   1.5 pixelscapes-dataset/scaled/JMC-CokeEcoAuctionRGB-17t.png\n",
            "   1.3 pixelscapes-dataset/scaled/LEE-skybox-12k.png\n",
            "   1.4 pixelscapes-dataset/scaled/MRS-MIA-Miami-Airport-96k.png\n",
            "   1.2 pixelscapes-dataset/scaled/BFG-N-ergie-Firmenzentrale-05s.png\n",
            "   2.2 pixelscapes-dataset/scaled/MCN-CokeCork-24s-CROP01.png\n",
            "   1.3 pixelscapes-dataset/scaled/ATK-centralfestivaleast-mall-31t.png\n",
            "   1.3 pixelscapes-dataset/scaled/CCN-Trafigura-white-papers-cover-14t.png\n",
            "   1.2 pixelscapes-dataset/scaled/BNM-bostonmag-22s.png\n",
            "   1.4 pixelscapes-dataset/scaled/CP9-ShopVille-53s.png\n",
            "   1.2 pixelscapes-dataset/scaled/RBM-Diggin-Season2-25k.png\n",
            "   2.2 pixelscapes-dataset/scaled/MCN-CokeDublin-43s-CROP01.png\n",
            "   1.2 pixelscapes-dataset/scaled/JWT-station-14k.png\n",
            "   1.1 pixelscapes-dataset/scaled/STR-CreationLuanda-2k.png\n",
            "   1.1 pixelscapes-dataset/scaled/TM3-Home-04s.png\n",
            "   1.3 pixelscapes-dataset/scaled/BFG-N-ergie-Heizkraftwerk-10s.png\n",
            "   1.4 pixelscapes-dataset/scaled/MRS-Miami-Pixorama-79t.png\n",
            "   1.2 pixelscapes-dataset/scaled/FTN-FutureNow-2017-cover-18k-half.png\n",
            "   1.2 pixelscapes-dataset/scaled/PT-SueddeutscheZ-FacebookCampus-08t.png\n",
            "   1.2 pixelscapes-dataset/scaled/LVT-Panoramic-Meiji-Shrine-14s.png\n",
            "   1.1 pixelscapes-dataset/scaled/KHT-ArlaFarm-Winter-05t-io.png\n",
            "   1.2 pixelscapes-dataset/scaled/WUN-xmas-14k.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-SanFrancisco-190k.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-ParisPoster-64t-1x.png\n",
            "   1.5 pixelscapes-dataset/scaled/OGY-Yahoo-Poster-38t.png\n",
            "   2.2 pixelscapes-dataset/scaled/MCN-CokeGalway-39s-CROP01.png\n",
            "   2.2 pixelscapes-dataset/scaled/MCN-CokeBelfast-33s-CROP01.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-Moscow-227k.png\n",
            "   1.4 pixelscapes-dataset/scaled/FTU-Yuzutown-68k.png\n",
            "   1.3 pixelscapes-dataset/scaled/EBY-Wallpaper-Pixorama-41t.png\n",
            "   1.4 pixelscapes-dataset/scaled/KBM-oslocentralXmas-34t.png\n",
            "   1.2 pixelscapes-dataset/scaled/TNY-holidaytrainshow-22k.png\n",
            "   1.2 pixelscapes-dataset/scaled/EBY-FooBar-35t.png\n",
            "   1.2 pixelscapes-dataset/scaled/LVT-Panoramic-TiliX-Omotesando-48t.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-Wallpaper-Cars-13t.png\n",
            "   1.4 pixelscapes-dataset/scaled/ECB-NY-V5-03t.png\n",
            "   1.3 pixelscapes-dataset/scaled/TM3-Battlefield-12s.png\n",
            "   1.4 pixelscapes-dataset/scaled/EBY-Rio-Poster-34k.png\n",
            "\n",
            "Creating non-existant output directory: pixelscapes-dataset/cropped\n",
            "\n",
            "Creating classes for crops from each input image...\n",
            "Generating random crops...\n",
            "Saved crop 19900/20000 at 550x371 from pixelscapes-dataset/scaled/MCS-singapore-19t.png to pixelscapes-dataset/cropped/00014/019900.png\u001b[KDone!\n"
          ]
        }
      ],
      "source": [
        "#@title <big><big><big>Prepare **Isometric Pixelart Dataset**</big></big></big> { vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#@markdown Prepare the training image set from my github repo by randomly cropping  pixelscapes.\n",
        "\n",
        "#@markdown Options:\n",
        "generate_missing_dataset = True #@param {type:\"boolean\"}\n",
        "#@markdown > Check this box to enable generating a new dataset from the images and script in my [pixelscapes-dataset repo](https://github.com/un1tz3r0/pixelscapes-dataset.git)\n",
        "force_regenerate_dataset = False #@param {type:\"boolean\"}\n",
        "#@markdown > Check this box to skip checking google drive for dataset.zip, and rebuild a new dataset from the pixelscapes-dataset repo's source images and randomcrops.py script. When done, the new dataset will be uploaded via rclone to Google Drive (as dataset.zip, an existing dataset.zip, if present, will be backed up)\n",
        "generate_dataset_count = 20000 #@param {type:\"integer\"}\n",
        "#@markdown > Size of the dataset to generate, in number of training images. These will be random crops from the source images, weighted by relative size so all pixels contribute equally to the training. When generating a new dataset from source images. output this many randomly cropped squares\n",
        "upscale_factor =  1.0#@param {type:\"number\", min:1.0, max:4.0}\n",
        "#@markdown > Zoom original images using a pretrained superresolution model with RealESRGAN by this factor before randomly cropping.\n",
        "weighting_amount =  0.25 #@param {type:\"number\", min:0.0, max:1.0}\n",
        "#@markdown > Amount of weighting based on source image size to use when sampling source images. 1.0=probability is proportional to ${width} \\times {height}$, 0.0 = even probability\n",
        "unzip_dataset = True #@param {type: \"boolean\"}\n",
        "#@markdown > Extract the dataset.zip to /content/dataset (needs patched StyleGAN3 train.py, which is used by this notebook already.)\n",
        "\n",
        "dataset_name = \"pixelscapes-256-{upscale_factor}x-{generate_dataset_count}\"\n",
        "\n",
        "drive_dataset_path = f\"/content/drive/MyDrive/pixelscapes-datasets/{dataset_name}\"\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.exists(drive_dataset_path):\n",
        "  if os.path.exists(\"/content/dataset\"):\n",
        "    !rm -Rfv \"/content/dataset\"\n",
        "  !cp -av $drive_dataset_path /content/dataset\n",
        "else:\n",
        "  # check if we need to generate a dataset from sources\n",
        "  if (((not os.path.exists(\"/content/dataset.zip\" )) and \\\n",
        "       (not os.path.exists(\"/content/dataset\"))) and \\\n",
        "      generate_missing_dataset) or force_regenerate_dataset:\n",
        "    print(\"Generating missing dataset from github repo now!\")\n",
        "    # create a new dataset.zip from the source images and randomcrop script in our github repo\n",
        "    %cd '/content/'\n",
        "\n",
        "    def install_realesrgan():\n",
        "      %cd '/content/'\n",
        "      # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "      !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "      %cd Real-ESRGAN\n",
        "      !pip3 install -r requirements.txt\n",
        "      !python3 setup.py develop --user\n",
        "      # Download the pre-trained model\n",
        "      !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "\n",
        "    def upscale_all(inputdir, outputdir, factor = 2.0):\n",
        "      import pathlib\n",
        "      outpath = pathlib.Path(outputdir).absolute()\n",
        "      if not (outpath.exists() and outpath.is_dir()):\n",
        "          outpath.mkdir()\n",
        "      infiles = [f.absolute() for f in pathlib.Path(inputdir).glob(\"*.png\")]\n",
        "      for f in infiles:\n",
        "        fin = str(f)\n",
        "        fo = outpath / f.name\n",
        "        if fo.exists():\n",
        "            print(f\"Skipping existing output file: {fo}\")\n",
        "            continue\n",
        "            #fo = outpath / f\"{fo.stem}-out.{fo.suffix}\"\n",
        "        fout = str(fo.parent.absolute())\n",
        "        print(f\"Upscaling x{factor}: {fin} -> {fout}\")\n",
        "        %cd /content/Real-ESRGAN\n",
        "        if factor != 1.0:\n",
        "          !python3 inference_realesrgan.py  -i $fin --outscale $factor -o $fout -n RealESRGAN_x4plus\n",
        "        else:\n",
        "          !ln -vs $fin $fout\n",
        "    \n",
        "    import os, pathlib\n",
        "    if not pathlib.Path(\"pixelscapes-dataset\").exists():\n",
        "      !git clone https://github.com/un1tz3r0/pixelscapes-dataset.git\n",
        "    else:\n",
        "      %cd /content/pixelscapes-dataset\n",
        "      !git diff --no-ext-diff --quiet --exit-code || rm -Rf cropped ../dataset.zip\n",
        "      !git pull\n",
        "    \n",
        "    %cd /content\n",
        "    if not os.path.exists(\"/content/pixelscapes-dataset/scaled/\"):\n",
        "    #if True:\n",
        "      print(\">>> Installing upscaler network to zoom 2x source images\")\n",
        "      install_realesrgan()\n",
        "      print(\">>> Upscaling raw dataset images...\")\n",
        "      upscale_all(\"/content/pixelscapes-dataset/pixelscapes/\", \\\n",
        "                  \"/content/pixelscapes-dataset/scaled\", upscale_factor)\n",
        "      print(\">>> Done, now cropping from upscaled images...\")\n",
        "    \n",
        "    %cd /content\n",
        "    if not os.path.exists(\"/content/pixelscapes-dataset/cropped\"):\n",
        "      !python3 pixelscapes-dataset/randomcrops.py \\\n",
        "        pixelscapes-dataset/scaled \\\n",
        "        pixelscapes-dataset/cropped \\\n",
        "        --count $generate_dataset_count \\\n",
        "        --size 256 --weighting $weighting_amount\n",
        "\n",
        "    if os.path.exists(\"/content/stylegan3/dataset_tool.py\"):\n",
        "      !python3 /content/stylegan3/dataset_tool.py \\\n",
        "        --source=pixelscapes-dataset/cropped \\\n",
        "        --dest=dataset.zip \\\n",
        "        --resolution='256x256'\n",
        "\n",
        "      datasetpath = \"/content/dataset.zip\"\n",
        "    else:\n",
        "      datasetpath = \"/content/dataset\"\n",
        "\n",
        "    if not os.path.exists(drive_dataset_path):\n",
        "      #!cp -av /content/pixelscapes-dataset/cropped $drive_dataset_path\n",
        "      pass\n",
        "\n",
        "    # upload the newly created dataset\n",
        "    #print(\"Syncing dataset.zip to drive...\")\n",
        "    #resultcode = rclone(\"syncto\", \"--progress\", \"/content/dataset.zip\", \"driveapi:/\", output=\"pass\", check=False)\n",
        "    #if resultcode != 0:\n",
        "    #  print(f\"... not synced, result code is {resultcode}\")\n",
        "    #else:\n",
        "    #  print(\"ok\")\n",
        "\n",
        "  '''\n",
        "  !rm -rf /content/dataset\n",
        "  # unzip the dataset.zip if needed and we have one\n",
        "  if unzip_dataset and pathlib.Path(\"/content/dataset.zip\").exists() and not (pathlib.Path(\"/content/dataset\").exists() and pathlib.Path(\"/content/dataset\").is_dir()):\n",
        "    print(\"Unzipping dataset.zip to /content/dataset/...\")\n",
        "    import ipywidgets\n",
        "    from IPython import display\n",
        "    outw = widgets.Output()\n",
        "    display.display(outw)\n",
        "    import subprocess\n",
        "    p = subprocess.Popen([\"unzip\", \"-d/content/dataset\", \"/content/dataset.zip\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
        "    buf = bytes()\n",
        "    lineno = 0\n",
        "    while p.returncode is None:\n",
        "      o, e = p.communicate(bytes())\n",
        "      buf = buf + o\n",
        "      lines = buf.splitlines()\n",
        "      buf = lines[-1]\n",
        "      lines = lines[0:-1]\n",
        "      for line in lines:\n",
        "        lineno = lineno + 1\n",
        "        if lineno > 100:\n",
        "          lineno = 0\n",
        "          outw.clear_output(wait=True)\n",
        "          with outw:\n",
        "            print(line.strip().decode(\"utf8\"), flush=True)\n",
        "    print(\"done!\")\n",
        "    datasetpath = \"/content/dataset\"\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2gIxZhw2me"
      },
      "source": [
        "# <big><big>Fine Tune</big></big>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqBBkqPjqESf"
      },
      "source": [
        "<!-- For gigachads. \n",
        "We're going to do what's called a pro-gamer move (or not): tune a small model, trained on rectilinear pixel art, on our own isometric pixelart dataset. Just because we can and it's much faster than training from scratch. \n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "-->\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here \n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there), and set width_height to 256x256, then run DD as usual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apH5i0hTqz1y",
        "outputId": "2248ac5e-11dd-495e-efaa-cd60f986016c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "| grad_norm  | 0.0377   |\n",
            "| loss       | 0.017    |\n",
            "| loss_q0    | 0.0408   |\n",
            "| loss_q1    | 0.0237   |\n",
            "| loss_q2    | 0.00547  |\n",
            "| loss_q3    | 0.000218 |\n",
            "| mse        | 0.0167   |\n",
            "| mse_q0     | 0.0396   |\n",
            "| mse_q1     | 0.0235   |\n",
            "| mse_q2     | 0.00542  |\n",
            "| mse_q3     | 0.000216 |\n",
            "| param_norm | 695      |\n",
            "| samples    | 4.44e+05 |\n",
            "| step       | 1.11e+05 |\n",
            "| vb         | 0.000351 |\n",
            "| vb_q0      | 0.00121  |\n",
            "| vb_q1      | 0.000173 |\n",
            "| vb_q2      | 4.8e-05  |\n",
            "| vb_q3      | 2.66e-06 |\n",
            "-------------------------\n",
            "-------------------------\n",
            "| grad_norm  | 0.0594   |\n",
            "| loss       | 0.0203   |\n",
            "| loss_q0    | 0.048    |\n",
            "| loss_q1    | 0.0276   |\n",
            "| loss_q2    | 0.00634  |\n",
            "| loss_q3    | 0.000346 |\n",
            "| mse        | 0.0192   |\n",
            "| mse_q0     | 0.0439   |\n",
            "| mse_q1     | 0.0274   |\n",
            "| mse_q2     | 0.00629  |\n",
            "| mse_q3     | 0.000342 |\n",
            "| param_norm | 695      |\n",
            "| samples    | 4.44e+05 |\n",
            "| step       | 1.11e+05 |\n",
            "| vb         | 0.00109  |\n",
            "| vb_q0      | 0.00402  |\n",
            "| vb_q1      | 0.000202 |\n",
            "| vb_q2      | 5.47e-05 |\n",
            "| vb_q3      | 4.06e-06 |\n",
            "-------------------------\n",
            "-------------------------\n",
            "| grad_norm  | 0.0716   |\n",
            "| loss       | 0.025    |\n",
            "| loss_q0    | 0.0539   |\n",
            "| loss_q1    | 0.0236   |\n",
            "| loss_q2    | 0.00502  |\n",
            "| loss_q3    | 0.00031  |\n",
            "| mse        | 0.0205   |\n",
            "| mse_q0     | 0.0404   |\n",
            "| mse_q1     | 0.0234   |\n",
            "| mse_q2     | 0.00498  |\n",
            "| mse_q3     | 0.000306 |\n",
            "| param_norm | 695      |\n",
            "| samples    | 4.44e+05 |\n",
            "| step       | 1.11e+05 |\n",
            "| vb         | 0.00453  |\n",
            "| vb_q0      | 0.0136   |\n",
            "| vb_q1      | 0.000173 |\n",
            "| vb_q2      | 4.46e-05 |\n",
            "| vb_q3      | 3.63e-06 |\n",
            "-------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "CHECKPOINT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "import shlex\n",
        "\n",
        "def latest_checkpoint(default_model):\n",
        "  import pathlib, os\n",
        "  try:\n",
        "    def kf(f):\n",
        "      return f.lstat().st_mtime\n",
        "    f = str(list(sorted(list(pathlib.Path(CHECKPOINT_PATH).glob(\"ema_0.9999_*.pt\")), key=kf))[-1])\n",
        "    print(f\"Resuming from latest checkpoint found: {f}\")\n",
        "    return f\n",
        "  except Exception as err:\n",
        "    print(f\"Error finding latest checkpoint in {CHECKPOINT_PATH}: {err}\")\n",
        "    print(f\"Resuming from default model: {default_model}\")\n",
        "    return default_model\n",
        "\n",
        "RESUME_CHECKPOINT=latest_checkpoint(\"/content/pixel_art_diffusion_soft_256.pt\")\n",
        "MODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\n",
        "DIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\n",
        "TRAIN_FLAGS=f\"--lr 2e-5 --batch_size 4 --save_interval 2000 --log_interval 50 --resume_checkpoint {shlex.quote(RESUME_CHECKPOINT)}\"\n",
        "DATASET_PATH=\"/content/pixelscapes-dataset/cropped/\" #change to point to your dataset path \n",
        "%cd /content/guided-diffusion-sxela\n",
        "!OPENAI_LOGDIR=$CHECKPOINT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Progress\n"
      ],
      "metadata": {
        "id": "ZpwP4RNcjVvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = []\n",
        "rows = []\n",
        "data = {}\n",
        "\n",
        "import csv\n",
        "with open('/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/progress.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for num, line in enumerate(reader):\n",
        "        if num not in rows:\n",
        "          rows.append(num)\n",
        "        for col, val in line.items():\n",
        "          if col not in data.keys():\n",
        "            data[col] = {}\n",
        "          data[col][num] = val\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "for colname, coldata in data.items():\n",
        "  plt.plot(list(coldata.keys()), list(coldata.values()), label=colname)\n",
        "#y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\n",
        "#y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\n",
        "#plt.plot(x, y1, label=\"line L\")\n",
        "#plt.plot(x, y2, label=\"line H\")\n",
        "plt.plot()\n",
        "\n",
        "plt.xlabel(\"x axis\")\n",
        "plt.ylabel(\"y axis\")\n",
        "plt.title(\"Line Graph Example\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "hj-P2VVAjexQ",
        "outputId": "7eb72752-b46c-4eb0-a8f8-72c5b7e31e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2eff57826177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udICgtfHEiQn"
      },
      "source": [
        "## Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion. \n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change model settings > custom model path to your ema checkpoint's location, as described in the previous cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cMKNlWF1VY"
      },
      "source": [
        "You can still sample using vanilla openai code, just plug your checkpoint in the cell below\n",
        "\n",
        "Don't forget to change all the paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-RCVDtuGArz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d84cf3e-22c0-4c94-b9f3-32d21d7c6442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"scripts/image_sample.py\", line 13, in <module>\n",
            "    from guided_diffusion import dist_util, logger\n",
            "  File \"/content/guided-diffusion-sxela/guided_diffusion/dist_util.py\", line 10, in <module>\n",
            "    from mpi4py import MPI\n",
            "ModuleNotFoundError: No module named 'mpi4py'\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = 'input some checkpoint path here' #use ema checkpoint\n",
        "!OPENAI_LOGDIR=/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS $DIFFUSION_FLAGS --timestep_respacing ddim100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFPy3r8AGEW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c02038-aacd-4885-b167-3f4639c8f39c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d2a7b959ca2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'some sample path'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marr_0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'some sample path'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avkq78LOjVhV"
      },
      "source": [
        "#Train (tune) 256x256 vanilla DD model\n",
        "Only if you have a beefy GPU with more than 16gb RAM\n",
        "\n",
        "For lvl 50 AI bosses, \n",
        "Will not fit into colab pro, only in colab pro+ with A100 gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk2sVBAxwurI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmMvZ9iDvwki"
      },
      "source": [
        "This mounts your google drive for easier storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db1AggwbvxAt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9d6U4_Gvz3v"
      },
      "source": [
        "This downloads the training code and installs it, then downloads a pre-trained model that we will be tuning on our dataset.\n",
        "\n",
        "I'm no using my edition of guided-diffusion in case you're going to use multiple GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jjf4ZopAwwyo"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/openai/guided-diffusion\n",
        "%cd /content/guided-diffusion  \n",
        "!pip install -e .\n",
        "!pip install mpi4py \n",
        "#is using on kaggle, replace !pip install mpi4py  with !conda install -y mpi4py\n",
        "#download model checkpoint\n",
        "!wget https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt -P /content/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ol_0nghwwGC"
      },
      "source": [
        "## Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_xc7GvAwgNU"
      },
      "source": [
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to. \n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here \n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there), \n",
        "\n",
        "you'll need to set custom model settings to this: \n",
        "\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJtcF4C_jDjz"
      },
      "outputs": [],
      "source": [
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64  --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50 --resume_checkpoint /content/256x256_diffusion_uncond.pt\"  \n",
        "DATASET_PATH=\"/content/YourDatasetHere/\" #change to point to your dataset path \n",
        "CHECKPOINT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion/\"\n",
        "%cd /content/guided-diffusion\n",
        "!OPENAI_LOGDIR=$CHECKPOINT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHbxCkynj2h0"
      },
      "source": [
        "Sample from model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NZ2Yi2CxITo"
      },
      "source": [
        "## Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion. \n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change settings like described in the previous cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fD1dA5vxRDb"
      },
      "source": [
        "You can still sample using vanilla openai code, just plug your checkpoint in the cell below\n",
        "\n",
        "Don't forget to change all the paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAZ1CwLkj11s"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'input some checkpoint path here'\n",
        "!OPENAI_LOGDIR=/content/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS --timestep_respacing ddim100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3cMMZLKkatO"
      },
      "source": [
        "Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WGeIjHhkbnr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiMreX-_n6Kz"
      },
      "source": [
        "# Train from scratch (smaller model)\n",
        "For lvl 1 AI crooks like me, should fit into colab pro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLR2sbXSoNdB"
      },
      "source": [
        "Train a smaller model that will fit definitely into colab pro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p6ThbjFxtBm"
      },
      "source": [
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to. \n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here \n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there), \n",
        "\n",
        "you'll need to set custom model settings to this: \n",
        "\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 128,\n",
        "        'num_heads': 4,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfH7XSbKn7ib"
      },
      "outputs": [],
      "source": [
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 128 --num_heads 4  --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50\"\n",
        "DATASET_PATH=\"/content/YourDatasetHere/\" #change to point to your dataset path \n",
        "CHECKPOINT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!OPENAI_LOGDIR=$CHECKPOINT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8seIEPF9pF7Q"
      },
      "source": [
        "### Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion. \n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change settings like described in the previous cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaHnukcKpEX-"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'input some checkpoint path here'\n",
        "!OPENAI_LOGDIR=/content/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS --timestep_respacing ddim100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mfZb81vpIK_"
      },
      "source": [
        "Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCwCF0NhpHy6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1NZ2Yi2CxITo",
        "CiMreX-_n6Kz"
      ],
      "name": "Training a Fine-Tuned Diffusion Model on Isometric Pixel Art.ipynb",
      "provenance": [],
      "background_execution": "on",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}