{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/un1tz3r0/finetunepixelartdiffusion/blob/main/fine_tuning_openai_diffusion_model_on_iso_pixelart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hBAjQO1kiEW"
      },
      "source": [
        "A simple colab to fine-tune openai diffusion models.\n",
        "\n",
        "With some modifications to use my (isometric pixelart dataset)[https://github.com/un1tz3r0/pixelscapes-dataset] and auto-resume from the most recent model snapshot found on google drive, or start with kaliyuga's soft pixel art diffusion model otherwise.\n",
        "\n",
        "Feel free to ask questions in this post's comments: https://www.patreon.com/posts/66246423 \n",
        "\n",
        "and questions specific to my pixelart fork here: https://github.com/un1tz3r0/finetunepixelartdiffusion \n",
        "\n",
        "by [Alex Spirin](https://twitter.com/devdef)\n",
        "\n",
        "iso pixelart fork by [Victor Condino](https://twitter.com/un1tz3r0)\n",
        "\n",
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=fine_tuning_openai_diffusion_model_on_iso_pixelart_ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ8BNzApp_Xk"
      },
      "source": [
        "#Train (tune) BEDROOM model :D\n",
        "Needs 16gb GPU RAM\n",
        "\n",
        "Works in colab pro and on kaggle "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufaUo7olwoF0"
      },
      "source": [
        "## Setup (run once per session)\n",
        "\n",
        "This mounts your google drive for easier storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtMv2MEzSzjN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg3mlCMIe1B6"
      },
      "source": [
        "This downloads the training code and installs it, then downloads a pre-trained model that we will be tuning on our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-fL3fb8wpxZ"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/Sxela/guided-diffusion-sxela\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd7VbOVO3626"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bECnYt6KXn8y"
      },
      "outputs": [],
      "source": [
        "#@title <big><big><big>Prepare **Isometric Pixelart Dataset**</big></big></big> { vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#@markdown Prepare the training image set from my github repo by randomly cropping  pixelscapes.\n",
        "\n",
        "#@markdown Options:\n",
        "generate_missing_dataset = True #@param {type:\"boolean\"}\n",
        "#@markdown > Check this box to enable generating a new dataset from the images and script in my [pixelscapes-dataset repo](https://github.com/un1tz3r0/pixelscapes-dataset.git)\n",
        "force_regenerate_dataset = False #@param {type:\"boolean\"}\n",
        "#@markdown > Check this box to skip checking google drive for dataset.zip, and rebuild a new dataset from the pixelscapes-dataset repo's source images and randomcrops.py script. When done, the new dataset will be uploaded via rclone to Google Drive (as dataset.zip, an existing dataset.zip, if present, will be backed up)\n",
        "generate_dataset_count = 20000 #@param {type:\"integer\"}\n",
        "#@markdown > Size of the dataset to generate, in number of training images. These will be random crops from the source images, weighted by relative size so all pixels contribute equally to the training. When generating a new dataset from source images. output this many randomly cropped squares\n",
        "upscale_factor =  1.0#@param {type:\"number\", min:1.0, max:4.0}\n",
        "#@markdown > Zoom original images using a pretrained superresolution model with RealESRGAN by this factor before randomly cropping.\n",
        "weighting_amount =  0.25 #@param {type:\"number\", min:0.0, max:1.0}\n",
        "#@markdown > Amount of weighting based on source image size to use when sampling source images. 1.0=probability is proportional to ${width} \\times {height}$, 0.0 = even probability\n",
        "unzip_dataset = True #@param {type: \"boolean\"}\n",
        "#@markdown > Extract the dataset.zip to /content/dataset (needs patched StyleGAN3 train.py, which is used by this notebook already.)\n",
        "\n",
        "dataset_name = \"pixelscapes-256-{upscale_factor}x-{generate_dataset_count}\"\n",
        "\n",
        "drive_dataset_path = f\"/content/drive/MyDrive/pixelscapes-datasets/{dataset_name}\"\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.exists(drive_dataset_path):\n",
        "  if os.path.exists(\"/content/dataset\"):\n",
        "    !rm -Rfv \"/content/dataset\"\n",
        "  !cp -av $drive_dataset_path /content/dataset\n",
        "else:\n",
        "  # check if we need to generate a dataset from sources\n",
        "  if (((not os.path.exists(\"/content/dataset.zip\" )) and \\\n",
        "       (not os.path.exists(\"/content/dataset\"))) and \\\n",
        "      generate_missing_dataset) or force_regenerate_dataset:\n",
        "    print(\"Generating missing dataset from github repo now!\")\n",
        "    # create a new dataset.zip from the source images and randomcrop script in our github repo\n",
        "    %cd '/content/'\n",
        "\n",
        "    def install_realesrgan():\n",
        "      %cd '/content/'\n",
        "      # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "      !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "      %cd Real-ESRGAN\n",
        "      !pip3 install -r requirements.txt\n",
        "      !python3 setup.py develop --user\n",
        "      # Download the pre-trained model\n",
        "      !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "\n",
        "    def upscale_all(inputdir, outputdir, factor = 2.0):\n",
        "      import pathlib\n",
        "      outpath = pathlib.Path(outputdir).absolute()\n",
        "      if not (outpath.exists() and outpath.is_dir()):\n",
        "          outpath.mkdir()\n",
        "      infiles = [f.absolute() for f in pathlib.Path(inputdir).glob(\"*.png\")]\n",
        "      for f in infiles:\n",
        "        fin = str(f)\n",
        "        fo = outpath / f.name\n",
        "        if fo.exists():\n",
        "            print(f\"Skipping existing output file: {fo}\")\n",
        "            continue\n",
        "            #fo = outpath / f\"{fo.stem}-out.{fo.suffix}\"\n",
        "        fout = str(fo.parent.absolute())\n",
        "        print(f\"Upscaling x{factor}: {fin} -> {fout}\")\n",
        "        %cd /content/Real-ESRGAN\n",
        "        if factor != 1.0:\n",
        "          !python3 inference_realesrgan.py  -i $fin --outscale $factor -o $fout -n RealESRGAN_x4plus\n",
        "        else:\n",
        "          !ln -vs $fin $fout\n",
        "    \n",
        "    import os, pathlib\n",
        "    if not pathlib.Path(\"pixelscapes-dataset\").exists():\n",
        "      !git clone https://github.com/un1tz3r0/pixelscapes-dataset.git\n",
        "    else:\n",
        "      %cd /content/pixelscapes-dataset\n",
        "      !git diff --no-ext-diff --quiet --exit-code || rm -Rf cropped ../dataset.zip\n",
        "      !git pull\n",
        "    \n",
        "    %cd /content\n",
        "    if not os.path.exists(\"/content/pixelscapes-dataset/scaled/\"):\n",
        "    #if True:\n",
        "      print(\">>> Installing upscaler network to zoom 2x source images\")\n",
        "      install_realesrgan()\n",
        "      print(\">>> Upscaling raw dataset images...\")\n",
        "      upscale_all(\"/content/pixelscapes-dataset/pixelscapes/\", \\\n",
        "                  \"/content/pixelscapes-dataset/scaled\", upscale_factor)\n",
        "      print(\">>> Done, now cropping from upscaled images...\")\n",
        "    \n",
        "    %cd /content\n",
        "    if not os.path.exists(\"/content/pixelscapes-dataset/cropped\"):\n",
        "      !python3 pixelscapes-dataset/randomcrops.py \\\n",
        "        pixelscapes-dataset/scaled \\\n",
        "        pixelscapes-dataset/cropped \\\n",
        "        --count $generate_dataset_count \\\n",
        "        --size 256 --weighting $weighting_amount\n",
        "\n",
        "    if os.path.exists(\"/content/stylegan3/dataset_tool.py\"):\n",
        "      !python3 /content/stylegan3/dataset_tool.py \\\n",
        "        --source=pixelscapes-dataset/cropped \\\n",
        "        --dest=dataset.zip \\\n",
        "        --resolution='256x256'\n",
        "\n",
        "      datasetpath = \"/content/dataset.zip\"\n",
        "    else:\n",
        "      datasetpath = \"/content/dataset\"\n",
        "\n",
        "    if not os.path.exists(drive_dataset_path):\n",
        "      #!cp -av /content/pixelscapes-dataset/cropped $drive_dataset_path\n",
        "      pass\n",
        "\n",
        "    # upload the newly created dataset\n",
        "    #print(\"Syncing dataset.zip to drive...\")\n",
        "    #resultcode = rclone(\"syncto\", \"--progress\", \"/content/dataset.zip\", \"driveapi:/\", output=\"pass\", check=False)\n",
        "    #if resultcode != 0:\n",
        "    #  print(f\"... not synced, result code is {resultcode}\")\n",
        "    #else:\n",
        "    #  print(\"ok\")\n",
        "\n",
        "  '''\n",
        "  !rm -rf /content/dataset\n",
        "  # unzip the dataset.zip if needed and we have one\n",
        "  if unzip_dataset and pathlib.Path(\"/content/dataset.zip\").exists() and not (pathlib.Path(\"/content/dataset\").exists() and pathlib.Path(\"/content/dataset\").is_dir()):\n",
        "    print(\"Unzipping dataset.zip to /content/dataset/...\")\n",
        "    import ipywidgets\n",
        "    from IPython import display\n",
        "    outw = widgets.Output()\n",
        "    display.display(outw)\n",
        "    import subprocess\n",
        "    p = subprocess.Popen([\"unzip\", \"-d/content/dataset\", \"/content/dataset.zip\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
        "    buf = bytes()\n",
        "    lineno = 0\n",
        "    while p.returncode is None:\n",
        "      o, e = p.communicate(bytes())\n",
        "      buf = buf + o\n",
        "      lines = buf.splitlines()\n",
        "      buf = lines[-1]\n",
        "      lines = lines[0:-1]\n",
        "      for line in lines:\n",
        "        lineno = lineno + 1\n",
        "        if lineno > 100:\n",
        "          lineno = 0\n",
        "          outw.clear_output(wait=True)\n",
        "          with outw:\n",
        "            print(line.strip().decode(\"utf8\"), flush=True)\n",
        "    print(\"done!\")\n",
        "    datasetpath = \"/content/dataset\"\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2gIxZhw2me"
      },
      "source": [
        "# <big><big>Fine Tune</big></big>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqBBkqPjqESf"
      },
      "source": [
        "<!-- For gigachads. \n",
        "We're going to do what's called a pro-gamer move (or not): tune a small model, trained on rectilinear pixel art, on our own isometric pixelart dataset. Just because we can and it's much faster than training from scratch. \n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "-->\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here \n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there), and set width_height to 256x256, then run DD as usual\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apH5i0hTqz1y"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "import shlex\n",
        "\n",
        "def latest_checkpoint(default_model):\n",
        "  import pathlib, os\n",
        "  try:\n",
        "    def kf(f):\n",
        "      return f.lstat().st_mtime\n",
        "    f = str(list(sorted(list(pathlib.Path(CHECKPOINT_PATH).glob(\"ema_0.9999_*.pt\")), key=kf))[-1])\n",
        "    print(f\"Resuming from latest checkpoint found: {f}\")\n",
        "    return f\n",
        "  except Exception as err:\n",
        "    print(f\"Error finding latest checkpoint in {CHECKPOINT_PATH}: {err}\")\n",
        "    print(f\"Resuming from default model: {default_model}\")\n",
        "    return default_model\n",
        "\n",
        "RESUME_CHECKPOINT=latest_checkpoint(\"/content/pixel_art_diffusion_soft_256.pt\")\n",
        "MODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\n",
        "DIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\n",
        "TRAIN_FLAGS=f\"--lr 2e-5 --batch_size 4 --save_interval 2000 --log_interval 50 --resume_checkpoint {shlex.quote(RESUME_CHECKPOINT)}\"\n",
        "DATASET_PATH=\"/content/pixelscapes-dataset/cropped/\" #change to point to your dataset path \n",
        "%cd /content/guided-diffusion-sxela\n",
        "!OPENAI_LOGDIR=$CHECKPOINT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udICgtfHEiQn"
      },
      "source": [
        "## Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion. \n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change model settings > custom model path to your ema checkpoint's location, as described in the previous cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57cMKNlWF1VY"
      },
      "source": [
        "You can still sample using vanilla openai code, just plug your checkpoint in the cell below\n",
        "\n",
        "Don't forget to change all the paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-RCVDtuGArz"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'input some checkpoint path here' #use ema checkpoint\n",
        "!OPENAI_LOGDIR=/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS $DIFFUSION_FLAGS --timestep_respacing ddim100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFPy3r8AGEW8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avkq78LOjVhV"
      },
      "source": [
        "#Train (tune) 256x256 vanilla DD model\n",
        "Only if you have a beefy GPU with more than 16gb RAM\n",
        "\n",
        "For lvl 50 AI bosses, \n",
        "Will not fit into colab pro, only in colab pro+ with A100 gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk2sVBAxwurI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmMvZ9iDvwki"
      },
      "source": [
        "This mounts your google drive for easier storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db1AggwbvxAt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9d6U4_Gvz3v"
      },
      "source": [
        "This downloads the training code and installs it, then downloads a pre-trained model that we will be tuning on our dataset.\n",
        "\n",
        "I'm no using my edition of guided-diffusion in case you're going to use multiple GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jjf4ZopAwwyo"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/openai/guided-diffusion\n",
        "%cd /content/guided-diffusion  \n",
        "!pip install -e .\n",
        "!pip install mpi4py \n",
        "#is using on kaggle, replace !pip install mpi4py  with !conda install -y mpi4py\n",
        "#download model checkpoint\n",
        "!wget https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt -P /content/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ol_0nghwwGC"
      },
      "source": [
        "## Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_xc7GvAwgNU"
      },
      "source": [
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to. \n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here \n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there), \n",
        "\n",
        "you'll need to set custom model settings to this: \n",
        "\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJtcF4C_jDjz"
      },
      "outputs": [],
      "source": [
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64  --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50 --resume_checkpoint /content/256x256_diffusion_uncond.pt\"  \n",
        "DATASET_PATH=\"/content/YourDatasetHere/\" #change to point to your dataset path \n",
        "CHECKPOINT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion/\"\n",
        "%cd /content/guided-diffusion\n",
        "!OPENAI_LOGDIR=$CHECKPOINT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHbxCkynj2h0"
      },
      "source": [
        "Sample from model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NZ2Yi2CxITo"
      },
      "source": [
        "## Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion. \n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change settings like described in the previous cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fD1dA5vxRDb"
      },
      "source": [
        "You can still sample using vanilla openai code, just plug your checkpoint in the cell below\n",
        "\n",
        "Don't forget to change all the paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAZ1CwLkj11s"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'input some checkpoint path here'\n",
        "!OPENAI_LOGDIR=/content/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS --timestep_respacing ddim100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3cMMZLKkatO"
      },
      "source": [
        "Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WGeIjHhkbnr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiMreX-_n6Kz"
      },
      "source": [
        "# Train from scratch (smaller model)\n",
        "For lvl 1 AI crooks like me, should fit into colab pro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLR2sbXSoNdB"
      },
      "source": [
        "Train a smaller model that will fit definitely into colab pro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p6ThbjFxtBm"
      },
      "source": [
        "Don't forget to change the paths:\n",
        "You need to change DATASET_PATH to point to your dataset images folder, and CHECKPOINT_PATH - to point to a folder you'd like to save progress to. \n",
        "\n",
        "For, example here /content/drive/MyDrive/deep_learning/guided-diffusion-sxela/ - this path points to a location, where all the training checkpoints will be saved\n",
        "\n",
        "and /content/YourDatasetHere/ - this path points to your dataset, i.e. a folder with images (no captions needed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will be using this model together with CLIP inside DiscoDiffusion, so we can train less, stop early and let CLIP do the heavy lifting.\n",
        "\n",
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n",
        "\n",
        "Validating means opening your CHECKPOINT_PATH folder, taking the ema_0.9999_(some number of steps).pt file with the highest number (the latest one), going to this version of DiscoDiffusion here \n",
        "https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb and setting this: diffusion-model - custom, custom_path - path to that ema file from the previous step (if you saved it on google drive - then just point it there), \n",
        "\n",
        "you'll need to set custom model settings to this: \n",
        "\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 128,\n",
        "        'num_heads': 4,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_checkpoint': use_checkpoint,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfH7XSbKn7ib"
      },
      "outputs": [],
      "source": [
        "MODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 128 --num_heads 4  --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\n",
        "TRAIN_FLAGS=\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50\"\n",
        "DATASET_PATH=\"/content/YourDatasetHere/\" #change to point to your dataset path \n",
        "CHECKPOINT_PATH=\"/content/drive/MyDrive/deep_learning/guided-diffusion-sxela/\"\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!OPENAI_LOGDIR=$CHECKPOINT_PATH python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $TRAIN_FLAGS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8seIEPF9pF7Q"
      },
      "source": [
        "### Sampling\n",
        "The best way to sample your model in real-life conditions is to plug it into DiscoDiffusion. \n",
        "\n",
        "\n",
        "Grab your latest ema checkpoint, open this colab here - https://github.com/Sxela/DiscoDiffusion-Warp/blob/main/Disco_Diffusion_v5_2_Warp_custom_model.ipynb\n",
        "\n",
        "and change settings like described in the previous cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaHnukcKpEX-"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'input some checkpoint path here'\n",
        "!OPENAI_LOGDIR=/content/samples/  python scripts/image_sample.py --num_samples 1 --model_path $checkpoint_path $MODEL_FLAGS --timestep_respacing ddim100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mfZb81vpIK_"
      },
      "source": [
        "Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCwCF0NhpHy6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "sample_path = 'some sample path'\n",
        "im = np.load(sample_path)\n",
        "PIL.Image.fromarray(im.f.arr_0[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1NZ2Yi2CxITo",
        "CiMreX-_n6Kz"
      ],
      "name": "fine-tuning openai diffusion model on iso pixelart.ipynb",
      "provenance": [],
      "background_execution": "on",
      "machine_shape": "hm",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}